{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "source": [
    "col_names = ['age', 'sex',\n",
    "             'cp','trestbps',\n",
    "             'chol','fbs',\n",
    "             'restecg','thalach','exang',\n",
    "             'oldpeak','slope',\n",
    "             'ca','thal','num']\n",
    "\"\"\" Attr explanation\n",
    "      -- 1. #3  (age)       \n",
    "      -- 2. #4  (sex)       \n",
    "      -- 3. #9  (cp) : chest pain type\n",
    "\t\t\t\t\t\t-- Value 1: typical angina\n",
    "\t\t\t\t\t\t-- Value 2: atypical angina\n",
    "\t\t\t\t\t\t-- Value 3: non-anginal pain\n",
    "\t\t\t\t\t\t-- Value 4: asymptomatic\n",
    "      -- 4. #10 (trestbps) : resting blood pressure (in mm Hg on admission to the hospital) \n",
    "      -- 5. #12 (chol): serum cholestoral in mg/dl      \n",
    "      -- 6. #16 (fbs): (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)       \n",
    "      -- 7. #19 (restecg): resting electrocardiographic results   \n",
    "\t\t\t\t\t\t-- Value 0: normal\n",
    "\t\t\t\t\t\t-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\t\t\t\t\t\t-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "      -- 8. #32 (thalach): maximum heart rate achieved   \n",
    "      -- 9. #38 (exang): exercise induced angina (1 = yes; 0 = no)     \n",
    "      -- 10. #40 (oldpeak):ST depression induced by exercise relative to rest   \n",
    "      -- 11. #41 (slope): the slope of the peak exercise ST segment     \n",
    "      -- 12. #44 (ca): number of major vessels (0-3) colored by flourosopy        \n",
    "      -- 13. #51 (thal): 3 = normal; 6 = fixed defect; 7 = reversable defect      \n",
    "      -- 14. #58 (num): diagnosis of heart disease (angiographic disease status) TARGET\n",
    "\t\t\t\t\t\t-- Value 0: < 50% diameter narrowing\n",
    "\t\t\t\t\t\t-- Value 1: > 50% diameter narrowing\n",
    "\"\"\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/'\n",
    "\n",
    "cleve_url = f'{url}processed.cleveland.data'\n",
    "\n",
    "cleve_df = pd.read_csv(cleve_url,index_col = False, names=col_names)\n",
    "cleve_df.columns.name = 'Cleveland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUICK DF LOOKUP\n",
    "def peek(frame):\n",
    "    print(f\"Here's a summary of the {frame.columns.name} dataset with dimensions {frame.shape}\")\n",
    "    print(frame.head())\n",
    "    print(\"-\"*80)\n",
    "    print(\"Its columns are:\")\n",
    "    print(frame.info())\n",
    "    print(\"-\"*80)\n",
    "    print(\"Here are its statistical characteristics:\")\n",
    "    print(frame.describe(include='all'))\n",
    "    #frame.groupby('num').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(method, frame, columns=None):\n",
    "\n",
    "    if method == 'std':\n",
    "        transformer = Pipeline(steps=[('standard', StandardScaler())])\n",
    "    if method == 'mn':\n",
    "        transformer = Pipeline(steps=[('minmax', MinMaxScaler())])\n",
    "    if columns == None:\n",
    "        columns = list(frame.columns)\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "            remainder='drop', #passthough features not listed\n",
    "            transformers=[\n",
    "                #('std', standard_transformer , ['thalach','oldpeak','chol']),\n",
    "                ('trnsf', transformer , columns)\n",
    "            ])\n",
    "    \n",
    "    processed_frame = pd.DataFrame(preprocessor.fit_transform(frame)) \n",
    "    processed_frame.columns = columns    \n",
    "    processed_frame.dropna(inplace=True)   \n",
    "    return processed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING UP THE FRAMES\n",
    "def preprocessing(frame):\n",
    "    try:\n",
    "        frame['thal'].replace({\"?\":np.nan}, inplace=True)\n",
    "        frame['ca'].replace({\"?\":np.nan}, inplace=True)\n",
    "        frame.dropna(subset = ['cp','ca','thal'],inplace=True)\n",
    "        frame[['cp','ca','thal']] = frame[['cp','ca','thal']].astype('category')\n",
    "        frame.loc[frame['num'] > 1, 'num'] = 1\n",
    "    except:\n",
    "        pass\n",
    "    frame = pd.get_dummies(frame)\n",
    "\n",
    "    cols = list(frame.columns.values) #Make a list of all of the columns in the df\n",
    "    cols.pop(cols.index('num')) #Remove b from list\n",
    "    frame = frame[cols+['num']] #Create new dataframe with columns in the wanted order\n",
    "\n",
    "\n",
    "    frame.dropna(inplace=True)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSIFICATION FNCTION WITH CROSSVAL BUILT-IN\n",
    "def classification_model(model, data, predictors, outcome, folds = 5, seed = randint(0, 5000)):\n",
    "    #Fit the model\n",
    "    model.fit(data[predictors],data[outcome])\n",
    "    \n",
    "    #Make predictions on training set\n",
    "    predictions = model.predict(data[predictors])\n",
    "\n",
    "    #Print accuracy\n",
    "    accuracy = accuracy_score(predictions,data[outcome])\n",
    "    print(f\"Accuracy: {accuracy*100:.4f}% (0:.3%)\")\n",
    "\n",
    "    #Perform k-fold cross-validation with 5 folds\n",
    "    kf = KFold(n_splits = folds, shuffle = True, random_state = seed)\n",
    "    error = []\n",
    "    for train,test in kf.split(data):\n",
    "        #Filter training data\n",
    "        train_predictors = (data[predictors].iloc[train,:])\n",
    "\n",
    "        #Target to train the algorithm\n",
    "        train_target = data[outcome].iloc[train]\n",
    "\n",
    "        #Training the algorithm using the predictors and target\n",
    "        model.fit(train_predictors,train_target)\n",
    "\n",
    "        #Record error from each cross-validation run\n",
    "        error.append(model.score(data[predictors].iloc[test,:],data[outcome].iloc[test]))\n",
    "        print(f\"Cross-Validation Score: {np.mean(error)*100:.4f}% ( 0:.3% )\")\n",
    "\n",
    "        #Fit the model again so it can be referred outside the function\n",
    "        model.fit(data[predictors],data[outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ranking(frame,k=5,print=False):\n",
    "    '''\n",
    "    Function for feature, giving Kbest and k-th most important features according to RandomForest\n",
    "    frame = dataframe to be evaluated\n",
    "    k = number of features\n",
    "    print = print out the sorted dictionary of features and their scores and importances\n",
    "\n",
    "    Function return a dict with the structure type: list of kth features. Capture them by choosing feature_ranking['scores'] or feature_ranking['importances']\n",
    "    \n",
    "    '''\n",
    "    #KBest set up\n",
    "    array = frame.values\n",
    "    X = array[:,0:-1]\n",
    "    Y = array[:,-1]\n",
    "    # feature extraction\n",
    "    kbest = SelectKBest(score_func=f_classif, k=k)\n",
    "    extracted = kbest.fit(X, Y)\n",
    "    # summarize scores\n",
    "    np.set_printoptions(precision=3)\n",
    "    #print(fit.scores_)\n",
    "    features = extracted.transform(X)\n",
    "    scores = list(extracted.scores_)\n",
    "    cols = list(frame.columns)\n",
    "    named_features = dict(zip(cols,scores))\n",
    "    sorted_scores = sorted(named_features.items(), key = lambda kv: kv[1], reverse= True)\n",
    "   \n",
    "    model = RandomForestClassifier(n_estimators=25, min_samples_split=25,max_depth=7,max_features=1)\n",
    "    model.fit(X,Y)\n",
    "    importances = dict(zip(list(frame.columns),model.feature_importances_) )\n",
    "    sorted_importances = sorted(importances.items(), key = lambda kv: kv[1], reverse= True)\n",
    "    \n",
    "    if print:\n",
    "        print(sorted_scores)\n",
    "        print(sorted_importances)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'scores':list(dict(sorted_scores[:k]).keys()), 'importances':list(dict(sorted_importances[:k]).keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleve_df = preprocessing(cleve_df)\n",
    "columns_to_scale = ['thalach', 'chol','oldpeak']\n",
    "scaled_columns = feature_scaling('std',cleve_df,columns_to_scale)\n",
    "\n",
    "for column in columns_to_scale:\n",
    "    cleve_df[column] = scaled_columns[column]\n",
    "cleve_df.dropna(inplace=True)\n",
    "print(feature_ranking(cleve_df)['importances'])\n",
    "\n",
    "#%%\n",
    "data = cleve_df\n",
    "\n",
    "#models\n",
    "logis = LogisticRegression(max_iter=800)\n",
    "rndf = RandomForestClassifier()\n",
    "supv = SVC()\n",
    "models = [logis,rndf,supv]\n",
    "\n",
    "#predictors\n",
    "corrs = ['thal_3.0','cp_4.0','ca_0.0','thal_7.0','exang','slope']\n",
    "scores = feature_ranking(cleve_df,5)['scores']\n",
    "importances = feature_ranking(cleve_df,5)['importances']\n",
    "predictors = [corrs,scores,importances]\n",
    "\n",
    "#target\n",
    "outcome = 'num'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for p in predictors:\n",
    "        print(model)\n",
    "        print(p)\n",
    "        print('-'*50)\n",
    "        classification_model(model,data,p,outcome,seed = 132)\n",
    "        print('-'*50)\n",
    "\n",
    "#Best model = supv\n",
    "#Best predictors = corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 2, 3], 'probability':[True]}\n",
    "clf = GridSearchCV(supv, parameters)\n",
    "clf.fit(data.iloc[:,0:-1], data.iloc[:,-1])\n",
    "supv = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "Y = data.iloc[:,-1]\n",
    "\n",
    "test_size = 0.15\n",
    "seed = 17\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n",
    "random_state=seed)\n",
    "model = supv.fit(X_train[corrs], Y_train)\n",
    "\n",
    "\n",
    "result = model.score(X_test[corrs], Y_test)\n",
    "print(result*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = supv.fit(X[corrs], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('heartd_clf.pkl', 'wb'))"
   ]
  }
 ]
}